
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>nntoolbox.sequence.components package &#8212; nn-toolbox 0.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="nntoolbox.sequence.components.cells package" href="nntoolbox.sequence.components.cells.html" />
    <link rel="prev" title="nntoolbox.sequence package" href="nntoolbox.sequence.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="nntoolbox-sequence-components-package">
<h1>nntoolbox.sequence.components package<a class="headerlink" href="#nntoolbox-sequence-components-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.sequence.components.cells.html">nntoolbox.sequence.components.cells package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nntoolbox.sequence.components.cells.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="nntoolbox.sequence.components.cells.html#module-nntoolbox.sequence.components.cells.highway">nntoolbox.sequence.components.cells.highway module</a></li>
<li class="toctree-l2"><a class="reference internal" href="nntoolbox.sequence.components.cells.html#module-nntoolbox.sequence.components.cells.multiplicative">nntoolbox.sequence.components.cells.multiplicative module</a></li>
<li class="toctree-l2"><a class="reference internal" href="nntoolbox.sequence.components.cells.html#module-nntoolbox.sequence.components.cells.on">nntoolbox.sequence.components.cells.on module</a></li>
<li class="toctree-l2"><a class="reference internal" href="nntoolbox.sequence.components.cells.html#module-nntoolbox.sequence.components.cells.zoneout">nntoolbox.sequence.components.cells.zoneout module</a></li>
<li class="toctree-l2"><a class="reference internal" href="nntoolbox.sequence.components.cells.html#module-nntoolbox.sequence.components.cells">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-nntoolbox.sequence.components.attention">
<span id="nntoolbox-sequence-components-attention-module"></span><h2>nntoolbox.sequence.components.attention module<a class="headerlink" href="#module-nntoolbox.sequence.components.attention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nntoolbox.sequence.components.attention.AdditiveAttention">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.attention.</code><code class="sig-name descname">AdditiveAttention</code><span class="sig-paren">(</span><em class="sig-param">key_dim: int</em>, <em class="sig-param">query_dim: int</em>, <em class="sig-param">value_dim: int</em>, <em class="sig-param">hidden_dim: int</em>, <em class="sig-param">return_summary: bool</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#AdditiveAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.AdditiveAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nntoolbox.sequence.components.attention.Attention" title="nntoolbox.sequence.components.attention.Attention"><code class="xref py py-class docutils literal notranslate"><span class="pre">nntoolbox.sequence.components.attention.Attention</span></code></a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.attention.AdditiveAttention.compute_scores">
<code class="sig-name descname">compute_scores</code><span class="sig-paren">(</span><em class="sig-param">keys</em>, <em class="sig-param">queries</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#AdditiveAttention.compute_scores"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.AdditiveAttention.compute_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the attention scores</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keys</strong> – a set of vectors with values’ info, to compute attention weights. (seq_length, n_batch, keys_dim)</p></li>
<li><p><strong>queries</strong> – query vectors. Shape (n_query, n_batch, query_dim)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The score for each time step. Shape (n_query, seq_length, n_batch, 1)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.attention.Attention">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.attention.</code><code class="sig-name descname">Attention</code><span class="sig-paren">(</span><em class="sig-param">key_dim: int</em>, <em class="sig-param">query_dim: int</em>, <em class="sig-param">value_dim: int</em>, <em class="sig-param">return_summary: bool</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#Attention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.Attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.attention.Attention.compute_attn_weights">
<code class="sig-name descname">compute_attn_weights</code><span class="sig-paren">(</span><em class="sig-param">keys: torch.Tensor</em>, <em class="sig-param">queries: torch.Tensor</em>, <em class="sig-param">mask=None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#Attention.compute_attn_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.Attention.compute_attn_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the attention weights</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keys</strong> – a set of vectors with values’ info, to compute attention weights. (seq_length, n_batch, keys_dim)</p></li>
<li><p><strong>queries</strong> – query vectors. Shape (n_query, n_batch, query_dim)</p></li>
<li><p><strong>mask</strong> – binary, indicating which vector is padding. Shape (seq_length, n_batch). dtype: uint8</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The weights for each time step. Shape (n_query, seq_length, n_batch, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nntoolbox.sequence.components.attention.Attention.compute_scores">
<code class="sig-name descname">compute_scores</code><span class="sig-paren">(</span><em class="sig-param">keys: torch.Tensor</em>, <em class="sig-param">queries: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#Attention.compute_scores"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.Attention.compute_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the attention scores</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keys</strong> – a set of vectors with values’ info, to compute attention weights. (seq_length, n_batch, keys_dim)</p></li>
<li><p><strong>queries</strong> – query vectors. Shape (n_query, n_batch, query_dim)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The score for each time step. Shape (n_query, seq_length, n_batch, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nntoolbox.sequence.components.attention.Attention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">keys: torch.Tensor</em>, <em class="sig-param">queries: torch.Tensor</em>, <em class="sig-param">values: torch.Tensor</em>, <em class="sig-param">mask=None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Any]<a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#Attention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.Attention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keys</strong> – a set of vectors with values’ info, to compute attention weights. (seq_length, n_batch, keys_dim)</p></li>
<li><p><strong>queries</strong> – queries vector. Shape (n_query, n_batch, query_dim)</p></li>
<li><p><strong>values</strong> – a set of vectors to be attended to. Shape (seq_length, n_batch, input_dim)</p></li>
<li><p><strong>mask</strong> – binary, indicating which vector is padding. Shape (seq_length, n_batch). dtype: uint8</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a weighted sum of the inputs. (n_query, n_batch, input_dim) if return_summary, else (n_query, seq_length, n_batch)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.attention.MultiplicativeAttention">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.attention.</code><code class="sig-name descname">MultiplicativeAttention</code><span class="sig-paren">(</span><em class="sig-param">key_dim</em>, <em class="sig-param">query_dim</em>, <em class="sig-param">value_dim</em>, <em class="sig-param">return_summary</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#MultiplicativeAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.MultiplicativeAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nntoolbox.sequence.components.attention.Attention" title="nntoolbox.sequence.components.attention.Attention"><code class="xref py py-class docutils literal notranslate"><span class="pre">nntoolbox.sequence.components.attention.Attention</span></code></a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.attention.MultiplicativeAttention.compute_scores">
<code class="sig-name descname">compute_scores</code><span class="sig-paren">(</span><em class="sig-param">keys</em>, <em class="sig-param">queries</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#MultiplicativeAttention.compute_scores"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.MultiplicativeAttention.compute_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the attention scores
:param keys: a set of vectors with values’ info, to compute attention weights. (seq_length, n_batch, keys_dim)
:param queries: query vectors. Shape (n_query, n_batch, query_dim)
:return: The score for each time step. Shape (n_query, seq_length, n_batch, 1)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.attention.ScaledDotProductAttention">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.attention.</code><code class="sig-name descname">ScaledDotProductAttention</code><span class="sig-paren">(</span><em class="sig-param">key_dim</em>, <em class="sig-param">query_dim</em>, <em class="sig-param">value_dim</em>, <em class="sig-param">return_summary</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#ScaledDotProductAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.ScaledDotProductAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nntoolbox.sequence.components.attention.Attention" title="nntoolbox.sequence.components.attention.Attention"><code class="xref py py-class docutils literal notranslate"><span class="pre">nntoolbox.sequence.components.attention.Attention</span></code></a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.attention.ScaledDotProductAttention.compute_scores">
<code class="sig-name descname">compute_scores</code><span class="sig-paren">(</span><em class="sig-param">keys</em>, <em class="sig-param">queries</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#ScaledDotProductAttention.compute_scores"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.ScaledDotProductAttention.compute_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the attention scores:
score(K, Q) = KQ^T / sqrt(d_k)
:param keys: a set of vectors with values’ info, to compute attention weights. (seq_length, batch_size, key_dim)
:param queries: query vectors. Shape (n_query, batch_size, query_dim = key_dim)
:return: The score for each time step. Shape (n_query, seq_length, n_batch, 1)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.attention.SelfAttention">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.attention.</code><code class="sig-name descname">SelfAttention</code><span class="sig-paren">(</span><em class="sig-param">base_attention</em>, <em class="sig-param">in_features: int</em>, <em class="sig-param">key_dim: int</em>, <em class="sig-param">query_dim: int</em>, <em class="sig-param">value_dim: int</em>, <em class="sig-param">value_as_key: bool = False</em>, <em class="sig-param">transform: bool = True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#SelfAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.SelfAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.attention.SelfAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Any]<a class="reference internal" href="../_modules/nntoolbox/sequence/components/attention.html#SelfAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.attention.SelfAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – (seq_length, batch_size, input_dim)</p></li>
<li><p><strong>lengths</strong> – (batch_size)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(seq_length, batch_size, input_dim) and mask (seq_len, batch_size)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nntoolbox.sequence.components.conv">
<span id="nntoolbox-sequence-components-conv-module"></span><h2>nntoolbox.sequence.components.conv module<a class="headerlink" href="#module-nntoolbox.sequence.components.conv" title="Permalink to this headline">¶</a></h2>
<p>Convolution and Quasi-RNN modules</p>
<dl class="class">
<dt id="nntoolbox.sequence.components.conv.ConvolutionalLayer1D">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.conv.</code><code class="sig-name descname">ConvolutionalLayer1D</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: Union[int, List[int]], kernel_sizes: List[int], strides: Union[int, List[int]] = 1, paddings: Union[int, List[int]] = 0, dilations: Union[int, List[int]] = 1, groups: Union[int, List[int]] = 1, biases: Union[bool, List[bool]] = True, padding_modes: Union[str, List[str]] = 'zeros', batch_first: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/conv.html#ConvolutionalLayer1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.conv.ConvolutionalLayer1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implement a layer that aggregates multiple conv1d of different filter sizes, then pooled over time.</p>
<p class="rubric">References</p>
<p>Yoon Kim. “Convolutional Neural Networks for Sentence Classification.”
<a class="reference external" href="https://aclweb.org/anthology/D14-1181">https://aclweb.org/anthology/D14-1181</a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.conv.ConvolutionalLayer1D.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/conv.html#ConvolutionalLayer1D.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.conv.ConvolutionalLayer1D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.conv.MaskedConv1D">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.conv.</code><code class="sig-name descname">MaskedConv1D</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/conv.html#MaskedConv1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.conv.MaskedConv1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.conv.Conv1d</span></code></p>
<p>Output at t should only depend on input from t - k + 1 to t</p>
<p class="rubric">References</p>
<p>James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher. “Quasi-Recurrent Neural Networks.”
<a class="reference external" href="https://arxiv.org/abs/1611.01576">https://arxiv.org/abs/1611.01576</a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.conv.MaskedConv1D.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/conv.html#MaskedConv1D.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.conv.MaskedConv1D.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> – (seq_len, batch_size, in_channels)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(seq_len, batch_size, out_channels)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.conv.QRNNLayer">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.conv.</code><code class="sig-name descname">QRNNLayer</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">pooling_mode: str = 'fo'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/conv.html#QRNNLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.conv.QRNNLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Quasi RNN layer. Decouple the gate computation (which can now be performed parallelwise with convolution)
and the hidden state sequential computation.</p>
<p class="rubric">References</p>
<p>James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher. “Quasi-Recurrent Neural Networks.”
<a class="reference external" href="https://arxiv.org/abs/1611.01576">https://arxiv.org/abs/1611.01576</a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.conv.QRNNLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">h: Optional[torch.Tensor] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]<a class="reference internal" href="../_modules/nntoolbox/sequence/components/conv.html#QRNNLayer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.conv.QRNNLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nntoolbox.sequence.components.embedding">
<span id="nntoolbox-sequence-components-embedding-module"></span><h2>nntoolbox.sequence.components.embedding module<a class="headerlink" href="#module-nntoolbox.sequence.components.embedding" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nntoolbox.sequence.components.embedding.AdditiveContextEmbedding">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.embedding.</code><code class="sig-name descname">AdditiveContextEmbedding</code><span class="sig-paren">(</span><em class="sig-param">num_embeddings</em>, <em class="sig-param">embedding_dim</em>, <em class="sig-param">padding_idx=None</em>, <em class="sig-param">max_norm=None</em>, <em class="sig-param">norm_type=2.0</em>, <em class="sig-param">scale_grad_by_freq=False</em>, <em class="sig-param">sparse=False</em>, <em class="sig-param">_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/embedding.html#AdditiveContextEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.embedding.AdditiveContextEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.sparse.Embedding</span></code></p>
<p>The embedding weights are fixed, except for a context vector c shared by all embedding:</p>
<p>embedding(x) = w_x + c</p>
<dl class="method">
<dt id="nntoolbox.sequence.components.embedding.AdditiveContextEmbedding.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/embedding.html#AdditiveContextEmbedding.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.embedding.AdditiveContextEmbedding.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.embedding.TiedOutputEmbedding">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.embedding.</code><code class="sig-name descname">TiedOutputEmbedding</code><span class="sig-paren">(</span><em class="sig-param">emb: torch.nn.modules.sparse.Embedding</em>, <em class="sig-param">bias: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/embedding.html#TiedOutputEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.embedding.TiedOutputEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Tie the weight of input embedding to the output layer</p>
<p class="rubric">References</p>
<p>Ofir Press and Lior Wolf. “Using the Output Embedding to Improve Language Models.”
<a class="reference external" href="https://arxiv.org/pdf/1608.05859.pdf">https://arxiv.org/pdf/1608.05859.pdf</a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.embedding.TiedOutputEmbedding.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/embedding.html#TiedOutputEmbedding.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.embedding.TiedOutputEmbedding.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.embedding.EmbeddingDropout">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.embedding.</code><code class="sig-name descname">EmbeddingDropout</code><span class="sig-paren">(</span><em class="sig-param">emb: torch.nn.modules.sparse.Embedding</em>, <em class="sig-param">drop_p: float = 0.5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/embedding.html#EmbeddingDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.embedding.EmbeddingDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>“Zero out” the same word across time step for each batch. E.g:</p>
<p>“The cat hates the dog.” -&gt; “- cat hates - dog.”</p>
<p>Based on fastai’s notebook implementation (which is slightly slower than as suggested in the paper, but easier
to implement).</p>
<p class="rubric">References</p>
<p>FastAI Course 2 V3 Notebook:
<a class="reference external" href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb">https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb</a></p>
<p>Yarin Gal and Zoubin Ghahramani. “A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.”
<a class="reference external" href="https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf">https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf</a></p>
<p>Stephen Merity, Nitish Shirish Keskar, Richard Socher. “Regularizing and Optimizing LSTM Language Models.”
<a class="reference external" href="https://arxiv.org/abs/1708.02182">https://arxiv.org/abs/1708.02182</a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.embedding.EmbeddingDropout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/embedding.html#EmbeddingDropout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.embedding.EmbeddingDropout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.embedding.SinusoidPositionalEncoding">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.embedding.</code><code class="sig-name descname">SinusoidPositionalEncoding</code><a class="reference internal" href="../_modules/nntoolbox/sequence/components/embedding.html#SinusoidPositionalEncoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.embedding.SinusoidPositionalEncoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Sinusoid Positional Encoding for Transformers. (UNTESTED)</p>
<p class="rubric">References</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.
“Attention Is All You Need.” <a class="reference external" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="nntoolbox.sequence.components.embedding.SinusoidPositionalEncoding.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/embedding.html#SinusoidPositionalEncoding.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.embedding.SinusoidPositionalEncoding.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> – (seq_len, batch_size, n_features)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>same shape</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nntoolbox.sequence.components.hierarchical">
<span id="nntoolbox-sequence-components-hierarchical-module"></span><h2>nntoolbox.sequence.components.hierarchical module<a class="headerlink" href="#module-nntoolbox.sequence.components.hierarchical" title="Permalink to this headline">¶</a></h2>
<p>Reference:
<a class="reference external" href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a></p>
<dl class="class">
<dt id="nntoolbox.sequence.components.hierarchical.TimeDistributed">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.hierarchical.</code><code class="sig-name descname">TimeDistributed</code><span class="sig-paren">(</span><em class="sig-param">module: torch.nn.modules.module.Module</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/hierarchical.html#TimeDistributed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.hierarchical.TimeDistributed" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Apply a module across time channel. Assume that 1st axis is the the major time axis, 2nd is the minor time axis,
and 3rd axis is batch size. (UNTESTED)</p>
<p>Example: major time axis is number of sentences per document; minor time axis is number of words per sentence;
batch size is number of documents in a batch</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – takes as input a sequence data type with shape (seq_length, batch_size, <a href="#id1"><span class="problematic" id="id2">*</span></a>)</p>
</dd>
</dl>
<dl class="method">
<dt id="nntoolbox.sequence.components.hierarchical.TimeDistributed.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/hierarchical.html#TimeDistributed.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.hierarchical.TimeDistributed.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> – (doc_length, sent_length, batch_size, <a href="#id3"><span class="problematic" id="id4">*</span></a>)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(doc_length, sent_length, batch_size, <a href="#id5"><span class="problematic" id="id6">*</span></a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nntoolbox.sequence.components.modifier">
<span id="nntoolbox-sequence-components-modifier-module"></span><h2>nntoolbox.sequence.components.modifier module<a class="headerlink" href="#module-nntoolbox.sequence.components.modifier" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nntoolbox.sequence.components.modifier.ModifiedStackedRNN">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.modifier.</code><code class="sig-name descname">ModifiedStackedRNN</code><span class="sig-paren">(</span><em class="sig-param">base_rnn</em>, <em class="sig-param">num_layers: int</em>, <em class="sig-param">input_size: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">bidirectional: bool</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/modifier.html#ModifiedStackedRNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.modifier.ModifiedStackedRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.modifier.ModifiedStackedRNN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">h_0=None</em>, <em class="sig-param">lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/modifier.html#ModifiedStackedRNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.modifier.ModifiedStackedRNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.modifier.ResidualRNN">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.modifier.</code><code class="sig-name descname">ResidualRNN</code><span class="sig-paren">(</span><em class="sig-param">base_rnn</em>, <em class="sig-param">num_layers: int</em>, <em class="sig-param">input_size: int</em>, <em class="sig-param">bidirectional: bool</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/modifier.html#ResidualRNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.modifier.ResidualRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nntoolbox.sequence.components.modifier.ModifiedStackedRNN" title="nntoolbox.sequence.components.modifier.ModifiedStackedRNN"><code class="xref py py-class docutils literal notranslate"><span class="pre">nntoolbox.sequence.components.modifier.ModifiedStackedRNN</span></code></a></p>
<p>StackedRNN with residual connections:</p>
<p>i_{l + 1} = o_l + i_l = i_l + f(i_l, h_{l - 1})</p>
<p>o_{l + 1} = f(i_{l + 1}, h_l)</p>
<dl class="method">
<dt id="nntoolbox.sequence.components.modifier.ResidualRNN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">h_0=None</em>, <em class="sig-param">lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/modifier.html#ResidualRNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.modifier.ResidualRNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.modifier.JitModifiedStackedRNN">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.modifier.</code><code class="sig-name descname">JitModifiedStackedRNN</code><span class="sig-paren">(</span><em class="sig-param">base_rnn</em>, <em class="sig-param">num_layers: int</em>, <em class="sig-param">input_size: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">bidirectional: bool</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/modifier.html#JitModifiedStackedRNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.modifier.JitModifiedStackedRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code></p>
<p>Jiter implementation using ScriptModule. Currently only works with GRU</p>
</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.modifier.JitResidualRNN">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.modifier.</code><code class="sig-name descname">JitResidualRNN</code><span class="sig-paren">(</span><em class="sig-param">base_rnn</em>, <em class="sig-param">num_layers: int</em>, <em class="sig-param">input_size: int</em>, <em class="sig-param">bidirectional: bool</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/modifier.html#JitResidualRNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.modifier.JitResidualRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nntoolbox.sequence.components.modifier.JitModifiedStackedRNN" title="nntoolbox.sequence.components.modifier.JitModifiedStackedRNN"><code class="xref py py-class docutils literal notranslate"><span class="pre">nntoolbox.sequence.components.modifier.JitModifiedStackedRNN</span></code></a></p>
<p>StackedRNN with residual connections:</p>
<p>i_{l + 1} = o_l + i_l = i_l + f(i_l, h_{l - 1})</p>
<p>o_{l + 1} = f(i_{l + 1}, h_l)</p>
<p>Jiter implementation using ScriptModule. Currently only works with GRU</p>
<dl class="method">
<dt id="nntoolbox.sequence.components.modifier.JitResidualRNN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">h_0=None</em>, <em class="sig-param">lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/modifier.html#JitResidualRNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.modifier.JitResidualRNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nntoolbox.sequence.components.pool">
<span id="nntoolbox-sequence-components-pool-module"></span><h2>nntoolbox.sequence.components.pool module<a class="headerlink" href="#module-nntoolbox.sequence.components.pool" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nntoolbox.sequence.components.pool.MaxOverTime">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.pool.</code><code class="sig-name descname">MaxOverTime</code><span class="sig-paren">(</span><em class="sig-param">batch_first: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/pool.html#MaxOverTime"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.pool.MaxOverTime" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implement max pooling over time</p>
<p class="rubric">References</p>
<p>Yoon Kim. “Convolutional Neural Networks for Sentence Classification.”
<a class="reference external" href="https://aclweb.org/anthology/D14-1181">https://aclweb.org/anthology/D14-1181</a></p>
<p>Ronan Collobert et al. “Natural Language Processing (Almost) from Scratch.”
<a class="reference external" href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf</a></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.pool.MaxOverTime.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/nntoolbox/sequence/components/pool.html#MaxOverTime.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.pool.MaxOverTime.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> – (batch_size, seq_len, <a href="#id7"><span class="problematic" id="id8">*</span></a>) if batch_first, else (seq_len, batch_size, <a href="#id9"><span class="problematic" id="id10">*</span></a>)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(batch_size, <a href="#id11"><span class="problematic" id="id12">*</span></a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nntoolbox.sequence.components.rnn">
<span id="nntoolbox-sequence-components-rnn-module"></span><h2>nntoolbox.sequence.components.rnn module<a class="headerlink" href="#module-nntoolbox.sequence.components.rnn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.RNNDropout">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">RNNDropout</code><span class="sig-paren">(</span><em class="sig-param">p</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#RNNDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.RNNDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.rnn.RNNDropout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input_unpacked: torch.Tensor</em>, <em class="sig-param">h_0: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#RNNDropout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.RNNDropout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.JitRNNDropout">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">JitRNNDropout</code><span class="sig-paren">(</span><em class="sig-param">p</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#JitRNNDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.JitRNNDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.JitRNNLayer">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">JitRNNLayer</code><span class="sig-paren">(</span><em class="sig-param">base_cell: Callable[..., torch.nn.modules.module.Module], *cell_args, inp_drop_p: float = 0.0, recurrent_drop_p: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#JitRNNLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.JitRNNLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code></p>
<p>Implement an RNN layer with jit script module</p>
<p class="rubric">References</p>
<p>The PyTorch Team. “Optimizing CUDA Recurrent Neural Networks with TorchScript.”
<a class="reference external" href="https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/">https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/</a></p>
<p>Yarin Gal, Zoubin Ghahramani. “A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.”
<a class="reference external" href="https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks">https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks</a></p>
</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.JitLSTMLayer">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">JitLSTMLayer</code><span class="sig-paren">(</span><em class="sig-param">base_cell: Callable[..., torch.nn.modules.module.Module], *cell_args, inp_drop_p: float = 0.0, recurrent_drop_p: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#JitLSTMLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.JitLSTMLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code></p>
<p>Implement an LSTM layer with jit script module</p>
<p class="rubric">References</p>
<p>The PyTorch Team. “Optimizing CUDA Recurrent Neural Networks with TorchScript.”
<a class="reference external" href="https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/">https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/</a></p>
<p>Yarin Gal, Zoubin Ghahramani. “A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.”
<a class="reference external" href="https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks">https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks</a></p>
</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.RNNSequential">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">RNNSequential</code><span class="sig-paren">(</span><em class="sig-param">*layers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#RNNSequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.RNNSequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="nntoolbox.sequence.components.rnn.RNNSequential.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">h_0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#RNNSequential.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.RNNSequential.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.JitRNNSequential">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">JitRNNSequential</code><span class="sig-paren">(</span><em class="sig-param">layers: List[torch.nn.modules.module.Module]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#JitRNNSequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.JitRNNSequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code></p>
<p>Implement a simple stacked RNN</p>
</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.JitResidualRNNV2">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">JitResidualRNNV2</code><span class="sig-paren">(</span><em class="sig-param">layers: List[torch.nn.modules.module.Module], skip_length: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#JitResidualRNNV2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.JitResidualRNNV2" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code></p>
<p>Implement a simple residual stacked RNN</p>
<p class="rubric">References</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi.
“Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.”
<a class="reference external" href="https://arxiv.org/pdf/1609.08144.pdf">https://arxiv.org/pdf/1609.08144.pdf</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layers</strong> – rnn layers. Must have output dimension the same as input dimension</p></li>
<li><p><strong>skip_length</strong> – length of the skip</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.JitLSTMSequential">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">JitLSTMSequential</code><span class="sig-paren">(</span><em class="sig-param">layers: List[torch.nn.modules.module.Module]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#JitLSTMSequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.JitLSTMSequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code></p>
<p>Implement a simple stacked LSTM</p>
</dd></dl>

<dl class="class">
<dt id="nntoolbox.sequence.components.rnn.JitResidualLSTMV2">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.rnn.</code><code class="sig-name descname">JitResidualLSTMV2</code><span class="sig-paren">(</span><em class="sig-param">layers: List[torch.nn.modules.module.Module], skip_length: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nntoolbox/sequence/components/rnn.html#JitResidualLSTMV2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.rnn.JitResidualLSTMV2" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code></p>
<p>Implement a simple residual stacked LSTM</p>
<p class="rubric">References</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi.
“Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.”
<a class="reference external" href="https://arxiv.org/pdf/1609.08144.pdf">https://arxiv.org/pdf/1609.08144.pdf</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layers</strong> – rnn layers. Must have output dimension the same as input dimension</p></li>
<li><p><strong>skip_length</strong> – length of the skip</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nntoolbox.sequence.components.transformer">
<span id="nntoolbox-sequence-components-transformer-module"></span><h2>nntoolbox.sequence.components.transformer module<a class="headerlink" href="#module-nntoolbox.sequence.components.transformer" title="Permalink to this headline">¶</a></h2>
<p>Some extra components for transformers model</p>
<dl class="class">
<dt id="nntoolbox.sequence.components.transformer.ProductKeyMemory">
<em class="property">class </em><code class="sig-prename descclassname">nntoolbox.sequence.components.transformer.</code><code class="sig-name descname">ProductKeyMemory</code><a class="reference internal" href="../_modules/nntoolbox/sequence/components/transformer.html#ProductKeyMemory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nntoolbox.sequence.components.transformer.ProductKeyMemory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
</dd></dl>

</div>
<div class="section" id="module-nntoolbox.sequence.components">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-nntoolbox.sequence.components" title="Permalink to this headline">¶</a></h2>
<p>For RNN, the hierarchy of components should be:</p>
<blockquote>
<div><p>Cell =&gt; Layer =&gt; StackedLayer</p>
</div></blockquote>
<p>Cell is the basic computing unit, processing every timestep. Layer takes a cell and apply it across the temporal
dimensions. StackedLayer stacks layers on top of each other and sequentially apply them in a depthwise manner.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">nn-toolbox</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.callbacks.html">nntoolbox.callbacks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.components.html">nntoolbox.components package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.ensembler.html">nntoolbox.ensembler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.graph.html">nntoolbox.graph package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.hooks.html">nntoolbox.hooks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.init.html">nntoolbox.init package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.losses.html">nntoolbox.losses package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.metrics.html">nntoolbox.metrics package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.models.html">nntoolbox.models package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.optim.html">nntoolbox.optim package</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="nntoolbox.sequence.html">nntoolbox.sequence package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="nntoolbox.sequence.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">nntoolbox.sequence.components package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#subpackages">Subpackages</a><ul>
<li class="toctree-l5"><a class="reference internal" href="nntoolbox.sequence.components.cells.html">nntoolbox.sequence.components.cells package</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components.attention">nntoolbox.sequence.components.attention module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components.conv">nntoolbox.sequence.components.conv module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components.embedding">nntoolbox.sequence.components.embedding module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components.hierarchical">nntoolbox.sequence.components.hierarchical module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components.modifier">nntoolbox.sequence.components.modifier module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components.pool">nntoolbox.sequence.components.pool module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components.rnn">nntoolbox.sequence.components.rnn module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components.transformer">nntoolbox.sequence.components.transformer module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nntoolbox.sequence.components">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="nntoolbox.sequence.learner.html">nntoolbox.sequence.learner package</a></li>
<li class="toctree-l3"><a class="reference internal" href="nntoolbox.sequence.models.html">nntoolbox.sequence.models package</a></li>
<li class="toctree-l3"><a class="reference internal" href="nntoolbox.sequence.searcher.html">nntoolbox.sequence.searcher package</a></li>
<li class="toctree-l3"><a class="reference internal" href="nntoolbox.sequence.utils.html">nntoolbox.sequence.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nntoolbox.sequence.html#module-nntoolbox.sequence">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.tabular.html">nntoolbox.tabular package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.transforms.html">nntoolbox.transforms package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.utils.html">nntoolbox.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.vision.html">nntoolbox.vision package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.visualization.html">nntoolbox.visualization package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nntoolbox.test.html">nntoolbox.test package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="nntoolbox.sequence.html">nntoolbox.sequence package</a><ul>
      <li>Previous: <a href="nntoolbox.sequence.html" title="previous chapter">nntoolbox.sequence package</a></li>
      <li>Next: <a href="nntoolbox.sequence.components.cells.html" title="next chapter">nntoolbox.sequence.components.cells package</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Nhat Pham.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/generated/nntoolbox.sequence.components.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>